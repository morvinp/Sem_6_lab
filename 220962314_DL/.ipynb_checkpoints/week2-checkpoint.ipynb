{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f902a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4610a77",
   "metadata": {},
   "source": [
    "## Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dedf40f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dz/da is  34.0\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor(1.,requires_grad = True)\n",
    "b = torch.tensor(2.,requires_grad = True)\n",
    "x = 2*a + 3*b\n",
    "y = 5*a*a + 3*b*b*b\n",
    "z = 2*x + 3*y\n",
    "\n",
    "z.backward()\n",
    "print(\"dz/da is \",a.grad.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5c8478",
   "metadata": {},
   "source": [
    "## Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e9ec0d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "da/dw 2.0\n"
     ]
    }
   ],
   "source": [
    "x1 = torch.tensor(2,requires_grad = True)\n",
    "w1 = torch.tensor(3.,requires_grad = True)\n",
    "b = torch.tensor(1.,requires_grad = True)\n",
    "\n",
    "u = w1*x1\n",
    "v = b+u\n",
    "a = torch.relu(v)\n",
    "a.backward()\n",
    "print('da/dw',w1.grad.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1638444",
   "metadata": {},
   "source": [
    "## Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "61485d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9991, grad_fn=<SigmoidBackward0>)\n",
      "da/dw 0.0018203349318355322\n"
     ]
    }
   ],
   "source": [
    "x1 = torch.tensor(2.,requires_grad = True)\n",
    "w1 = torch.tensor(3.,requires_grad = True)\n",
    "b = torch.tensor(1.,requires_grad = True)\n",
    "\n",
    "u = w1*x1\n",
    "v = b+u\n",
    "a = torch.sigmoid(v)\n",
    "print(a)\n",
    "a.backward()\n",
    "print('da/dw',w1.grad.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac40fad9",
   "metadata": {},
   "source": [
    "## Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbc349a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "By pytorch: -1.862145722952846e-06\n",
      "Analytically:  -1.8621456092660083e-06\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(3.,requires_grad = True)\n",
    "f = torch.exp(-x**2 - 2*x - torch.sin(x))\n",
    "f.backward()\n",
    "\n",
    "ana = -torch.exp(-x**2 - 2*x - torch.sin(x))*(2*x+2+torch.cos(x))\n",
    "print(\"By pytorch:\", x.grad.item())\n",
    "print(\"Analytically: \",ana.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2df931",
   "metadata": {},
   "source": [
    "## Q5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71ce0ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326.0\n",
      "326.0\n"
     ]
    }
   ],
   "source": [
    "x=torch.tensor(2.0, requires_grad=True)\n",
    "y=8*x**4+3*x**3+7*x**2+6*x+3\n",
    "y.backward()\n",
    "print(x.grad.item())\n",
    "y = 32*x**3 + 9*x**2 + 14*x + 6\n",
    "print(y.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e6fd44",
   "metadata": {},
   "source": [
    "## Q6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d254d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "55104491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch gradient w.r.t y: 0.0314033180475235\n",
      "Analytical gradient w.r.t y: 0.03140356324670285\n",
      "a=4.0, b=0.14112000167369843, c=28.34467124938965, d=28.34467124938965, e=3.3791110515594482, f=0.997680127620697\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "def forward_pass(x, y, z):\n",
    "    a = 2 * x\n",
    "    b = torch.sin(y)\n",
    "    c = a / b\n",
    "    d = c * z\n",
    "    e = torch.log(d + 1)\n",
    "    f = torch.tanh(e)\n",
    "    return f, a, b, c, d, e\n",
    "\n",
    "def analytical_gradient(x, y, z):\n",
    "    a = 2 * x\n",
    "    b = math.sin(y)\n",
    "    c = a / b\n",
    "    d = c * z\n",
    "    e = math.log(d + 1)\n",
    "    f = math.tanh(e)\n",
    "    df_de = 1 - f**2\n",
    "    de_dd = 1 / (d + 1)\n",
    "    dd_dc = z\n",
    "    dc_db = -a / b**2\n",
    "    db_dy = math.cos(y)\n",
    "    return df_de * de_dd * dd_dc * dc_db * db_dy\n",
    "\n",
    "def verify_gradient():\n",
    "    x = torch.tensor(2.0, requires_grad=True)\n",
    "    y = torch.tensor(3., requires_grad=True)\n",
    "    z = torch.tensor(1.0, requires_grad=True)\n",
    "\n",
    "    f, a, b, c, d, e = forward_pass(x, y, z)\n",
    "    f.backward()\n",
    "\n",
    "    print(\"PyTorch gradient w.r.t y:\", y.grad.item())\n",
    "    print(\"Analytical gradient w.r.t y:\", analytical_gradient(x.item(), y.item(), z.item()))\n",
    "    print(f\"a={a.item()}, b={b.item()}, c={c.item()}, d={d.item()}, e={e.item()}, f={f.item()}\")\n",
    "\n",
    "verify_gradient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3f8c015b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.1340)\n",
      "0.10434979945421219\n",
      "-0.24801760911941528\n",
      "0.08780732750892639\n",
      "0.08780732750892639\n",
      "0.29650694131851196\n",
      "1.0\n",
      "-0.0005669945701194567\n"
     ]
    }
   ],
   "source": [
    "# x = torch.tensor(1., requires_grad = True)\n",
    "# y = torch.tensor(1., requires_grad = True)\n",
    "# b = torch.sin(y) ; b.retain_grad()\n",
    "# a = 2*x ; a.retain_grad()\n",
    "# c = a/b ; c.retain_grad()\n",
    "# z = torch.tensor(1., requires_grad = True)\n",
    "# d = c*z ;d.retain_grad()\n",
    "# e = torch.log(d+1) ; e.retain_grad()\n",
    "# f = torch.tanh(e) ; f.retain_grad() \n",
    "# f.backward(retain_graph = True)\n",
    "# print(y.grad)\n",
    "# print(a.grad.item())\n",
    "# print(b.grad.item())\n",
    "# print(c.grad.item())\n",
    "# print(d.grad.item())\n",
    "# print(e.grad.item())\n",
    "# print(f.grad.item())\n",
    "# print(b.grad.item()*c.grad.item()*d.grad.item()*e.grad.item()*f.grad.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1c026229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f_e_grad: 1.0\n",
      "e_d_grad: 0.29650694131851196\n",
      "d_c_grad: 0.08780732750892639\n",
      "c_b_grad: -0.24801760911941528\n",
      "b_y_grad: -0.24801760911941528\n",
      "Manual grad by multiplying all intermediate gradients: 0.001601513708010316\n",
      "y.grad: -0.13400448858737946\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor(1., requires_grad=True)\n",
    "y = torch.tensor(1., requires_grad=True)\n",
    "\n",
    "# Forward pass operations\n",
    "b = torch.sin(y)\n",
    "b.retain_grad()  # Retain gradient for b\n",
    "a = 2*x\n",
    "a.retain_grad()  # Retain gradient for a\n",
    "c = a / b\n",
    "c.retain_grad()  # Retain gradient for c\n",
    "z = torch.tensor(1., requires_grad=True)\n",
    "d = c * z\n",
    "d.retain_grad()  # Retain gradient for d\n",
    "e = torch.log(d + 1)\n",
    "e.retain_grad()  # Retain gradient for e\n",
    "f = torch.tanh(e)\n",
    "f.retain_grad()  # Retain gradient for f\n",
    "\n",
    "# Perform backward pass to compute gradients\n",
    "f.backward(retain_graph=True)\n",
    "\n",
    "# Now we manually compute the gradient of y using the retained gradients\n",
    "\n",
    "# Intermediate gradients from the backward pass\n",
    "f_e_grad = f.grad  # df/de\n",
    "e_d_grad = e.grad  # de/dd\n",
    "d_c_grad = d.grad  # dd/dc\n",
    "c_a_grad = c.grad  # dc/da\n",
    "c_b_grad = b.grad  # dc/db\n",
    "b_y_grad = b.grad  # db/dy\n",
    "\n",
    "# Applying the chain rule manually\n",
    "# Correctly applying the chain rule\n",
    "# df/dy = (df/de) * (de/dd) * (dd/dc) * (dc/db) * (db/dy)\n",
    "\n",
    "manual_grad = (\n",
    "    f_e_grad * e_d_grad * d_c_grad * c_b_grad * b_y_grad\n",
    ")\n",
    "\n",
    "# Print individual gradients and the final manually computed gradient\n",
    "print(\"f_e_grad:\", f_e_grad.item())\n",
    "print(\"e_d_grad:\", e_d_grad.item())\n",
    "print(\"d_c_grad:\", d_c_grad.item())\n",
    "print(\"c_b_grad:\", c_b_grad.item())\n",
    "print(\"b_y_grad:\", b_y_grad.item())\n",
    "print(\"Manual grad by multiplying all intermediate gradients:\", manual_grad.item())\n",
    "\n",
    "# Final y.grad value for comparison\n",
    "print(\"y.grad:\", y.grad.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16806a0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
